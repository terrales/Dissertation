{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Analysis \n",
    "### The first part of this notebook comes from QMSum (Zhong et al. 2021)\n",
    "### The Data Analysis and Statistics section was created for the purpose of this disseratation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import json\n",
    "\n",
    "# read the dataset\n",
    "# please enter the path of your data\n",
    "def read_data(this_path, this_split):\n",
    "    split = this_split\n",
    "    data_path = this_path + split + '.jsonl'\n",
    "    data = []\n",
    "    with builtins.open(data_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    n_meetings = len(data)\n",
    "    print('Total {} meetings in the {} set.'.format(n_meetings, split))\n",
    "    return data\n",
    "\n",
    "path = \"../Data/QMSum/data/ALL/jsonl/\"\n",
    "split = 'train'\n",
    "data = read_data(path, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk\n",
    "from nltk import word_tokenize\n",
    "# tokneize a sent\n",
    "def tokenize(sent):\n",
    "    separator = ' '\n",
    "    tokens = separator.join(word_tokenize(sent.lower(), preserve_line=True))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some noises caused by speech recognition\n",
    "def clean_data(text):\n",
    "    text = text.replace('{ vocalsound } ', '')\n",
    "    text = text.replace('{ disfmarker } ', '')\n",
    "    text = text.replace('a_m_i_', 'ami')\n",
    "    text = text.replace('l_c_d_', 'lcd')\n",
    "    text = text.replace('p_m_s', 'pms')\n",
    "    text = text.replace('t_v_', 'tv')\n",
    "    text = text.replace('{ pause } ', '')\n",
    "    text = text.replace('{ nonvocalsound } ', '')\n",
    "    text = text.replace('{ gap } ', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From QMSum\n",
    "\n",
    "# process data for BART\n",
    "# the input of the model here is the entire content of the meeting\n",
    "bart_data = []\n",
    "for i in range(len(data)):\n",
    "    # get meeting content\n",
    "    src = []\n",
    "    for k in range(len(data[i]['meeting_transcripts'])):\n",
    "        cur_turn = data[i]['meeting_transcripts'][k]['speaker'].lower() + ': '\n",
    "        cur_turn = cur_turn + tokenize(data[i]['meeting_transcripts'][k]['content'])\n",
    "        src.append(cur_turn)\n",
    "    src = ' '.join(src)\n",
    "    for j in range(len(data[i]['general_query_list'])):\n",
    "        cur = {}\n",
    "        query = tokenize(data[i]['general_query_list'][j]['query'])\n",
    "        cur['src'] = clean_data('<s> ' + query + ' </s> ' + src + ' </s>')\n",
    "        target = tokenize(data[i]['general_query_list'][j]['answer'])\n",
    "        cur['tgt'] = target\n",
    "        bart_data.append(cur)\n",
    "    for j in range(len(data[i]['specific_query_list'])):\n",
    "        cur = {}\n",
    "        query = tokenize(data[i]['specific_query_list'][j]['query'])\n",
    "        cur['src'] = clean_data('<s> ' + query + ' </s> ' + src + ' </s>')\n",
    "        target = tokenize(data[i]['specific_query_list'][j]['answer'])\n",
    "        cur['tgt'] = target\n",
    "        bart_data.append(cur)\n",
    "        \n",
    "print('Total {} query-summary pairs in the {} set'.format(len(bart_data), split))\n",
    "print(bart_data[2])\n",
    "with open('data/bart_' + split + '.jsonl', 'w') as f:\n",
    "    for i in range(len(bart_data)):\n",
    "        print(json.dumps(bart_data[i]), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From QMSum\n",
    "\n",
    "# process data for BART\n",
    "# the input of the model here is the gold span corresponding to each query\n",
    "bart_data_gold = []\n",
    "for i in range(len(data)):\n",
    "    # get meeting content\n",
    "    entire_src = []\n",
    "    for k in range(len(data[i]['meeting_transcripts'])):\n",
    "        cur_turn = data[i]['meeting_transcripts'][k]['speaker'].lower() + ': '\n",
    "        cur_turn = cur_turn + tokenize(data[i]['meeting_transcripts'][k]['content'])\n",
    "        entire_src.append(cur_turn)\n",
    "    entire_src = ' '.join(entire_src)\n",
    "    for j in range(len(data[i]['general_query_list'])):\n",
    "        cur = {}\n",
    "        query = tokenize(data[i]['general_query_list'][j]['query'])\n",
    "        cur['src'] = clean_data('<s> ' + query + ' </s> ' + entire_src + ' </s>')\n",
    "        target = tokenize(data[i]['general_query_list'][j]['answer'])\n",
    "        cur['tgt'] = target\n",
    "        bart_data_gold.append(cur)\n",
    "    for j in range(len(data[i]['specific_query_list'])):\n",
    "        cur = {}\n",
    "        query = tokenize(data[i]['specific_query_list'][j]['query'])\n",
    "        src = []\n",
    "        # get the content in the gold span for each query\n",
    "        for span in data[i]['specific_query_list'][j]['relevant_text_span']:\n",
    "            assert len(span) == 2\n",
    "            st, ed = int(span[0]), int(span[1])\n",
    "            for k in range(st, ed + 1):\n",
    "                cur_turn = data[i]['meeting_transcripts'][k]['speaker'].lower() + ': '\n",
    "                cur_turn = cur_turn + tokenize(data[i]['meeting_transcripts'][k]['content'])\n",
    "                src.append(cur_turn)\n",
    "        src = ' '.join(src)\n",
    "        cur['src'] = clean_data('<s> ' + query + ' </s> ' + src + ' </s>')\n",
    "        target = tokenize(data[i]['specific_query_list'][j]['answer'])\n",
    "        cur['tgt'] = target\n",
    "        bart_data_gold.append(cur)\n",
    "        \n",
    "print('Total {} query-summary pairs in the {} set'.format(len(bart_data_gold), split))\n",
    "print(bart_data_gold[2])\n",
    "with open('data/bart_' + split + '._gold.jsonl', 'w') as f:\n",
    "    for i in range(len(bart_data_gold)):\n",
    "        print(json.dumps(bart_data_gold[i]), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse data \n",
    "\n",
    "#useful imports \n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# function for analysis of a single meeting transcript\n",
    "def analyse_meeting(data):\n",
    "    n_turns = len(data['meeting_transcripts'])\n",
    "    speakers_list = []\n",
    "    turn_len = []\n",
    "    all_content_tokens = []\n",
    "    for k in range(n_turns):\n",
    "        cur_speaker = data['meeting_transcripts'][k]['speaker'].lower()\n",
    "        if cur_speaker not in speakers_list:\n",
    "            speakers_list.append(cur_speaker)\n",
    "        tokenized_content = nltk.word_tokenize(tokenize(data['meeting_transcripts'][k]['content'].lower()))\n",
    "        len((tokenized_content))\n",
    "        turn_len.append(len((tokenized_content)))\n",
    "\n",
    "        all_content_tokens += [w for w in clean_data(tokenized_content) ]\n",
    "    n_speakers = len(speakers_list)\n",
    "    avg_turn_len = np.mean(turn_len)\n",
    "    n_tokens = len(all_content_tokens)\n",
    "    n_types = len(set(all_content_tokens))\n",
    "   # freq_dist = nltk.FreqDist(all_content_tokens)\n",
    "   # print(n_turns, n_speakers, avg_turn_len, n_tokens, n_types)\n",
    "    return pd.DataFrame.from_dict({'n_speakers':[n_speakers], 'n_turns':[n_turns], 'avg_turn_len':[avg_turn_len], \n",
    "                                    'n_tokens':[n_tokens], 'n_types':[n_types]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# analyse meeting in a directory (Academic, Committee, Product)\n",
    "def analyse_meeting_type(type):\n",
    "    # create df to store information\n",
    "    df = pd.DataFrame(columns=['n_speakers', 'n_turns', 'avg_turn_len', 'n_tokens', 'n_types'])\n",
    "    indices = []\n",
    "    # assign directory\n",
    "    directory = '../Data/QMSum/data/' + type + '/all/' # if type AVENI, change directory to \"../Data/Aveni/all/\"\n",
    "    # iterate over files in that directory\n",
    "    for filename in os.listdir(directory):\n",
    "        indices.append(filename)\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "        # print(f)\n",
    "            with open(f, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                new_row = analyse_meeting(data)\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df.index = indices\n",
    "\n",
    "    print(type)\n",
    "    print(df.head())\n",
    "    print()\n",
    "    # get average number of speakers per meeting\n",
    "    print('average number of speakers per meeting: ', df['n_speakers'].mean())\n",
    "\n",
    "    # get average number of turns per meeting \n",
    "    print('average number of turns per meeting: ', df['n_turns'].mean())\n",
    "\n",
    "    # get average turn length (in tokens) \n",
    "    print('average turn length in tokens: ',  df['avg_turn_len'].mean())\n",
    "\n",
    "    # get average number of tokens\n",
    "    print('average number of tokens: ', df['n_tokens'].mean())\n",
    "\n",
    "    # get average number of types\n",
    "    print('average number of different tokens (types): ', df['n_types'].mean())\n",
    "    print()\n",
    "\n",
    "\n",
    "def analyse_summaries(type):\n",
    "    df = pd.DataFrame(columns=['n_queries', 'sum_len'])\n",
    "    indices = []\n",
    "    # assign directory\n",
    "    directory = '../Data/QMSum/data/' + type + '/all/' # if type AVENI, change directory to \"../Data/Aveni/all/\"\n",
    "    # iterate over files in that directory\n",
    "    for filename in os.listdir(directory):\n",
    "        indices.append(filename)\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "        # print(f)\n",
    "            with open(f, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                n_gen_queries = len(data['general_query_list']) \n",
    "                n_spec_queries = len(data['specific_query_list'])\n",
    "                n_queries = n_gen_queries + n_spec_queries\n",
    "\n",
    "\n",
    "                gen_sum_len = [len(nltk.word_tokenize(tokenize(data['general_query_list'][k]['answer']))) for k in range(n_gen_queries)]\n",
    "                print(min(gen_sum_len))\n",
    "                print(max(gen_sum_len))\n",
    "                spec_sum_len = [len(nltk.word_tokenize(tokenize(data['specific_query_list'][k]['answer']))) for k in range(n_spec_queries)]\n",
    "                print(min(spec_sum_len))\n",
    "                print(max(spec_sum_len))\n",
    "                sum_len = np.mean(gen_sum_len + spec_sum_len)\n",
    "\n",
    "                new_row = pd.DataFrame.from_dict({'n_queries':[n_queries], 'sum_len':[sum_len]})\n",
    "                df = pd.concat([df, new_row], ignore_index=True)           \n",
    "    df.index = indices\n",
    "\n",
    "    print(type)\n",
    "    print(df.head())\n",
    "    print()\n",
    "    # get average number of queries per meeting\n",
    "    print('average number of queries per meeting: ', df['n_queries'].mean())\n",
    "\n",
    "    # get average length of summary\n",
    "    print('average length of summary: ', df['sum_len'].mean())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_meeting_type('Academic')\n",
    "analyse_meeting_type('Committee')\n",
    "analyse_meeting_type('Product')\n",
    "\n",
    "analyse_meeting_type('AVENI')\n",
    "\n",
    "\n",
    "analyse_summaries('AVENI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
